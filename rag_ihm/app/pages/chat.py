import os

import requests
import streamlit as st

# --- CONFIGURATION DE LA PAGE ---
st.set_page_config(page_title="IsiDore", page_icon="ü§ñ", layout="centered")

# --- STYLES CSS ---
st.markdown(
    """
    <style>
    .stChatMessage { border-radius: 15px; margin-bottom: 10px; }
    .stButton>button { border-radius: 8px; }
    .stExpander { border: none; box-shadow: none; background-color: transparent; }
    </style>
    """,
    unsafe_allow_html=True,
)

# --- VARIABLES D'ENVIRONNEMENT ---
RAG_API_TEST_CONNEXION_URL = os.getenv("RAG_API_TEST_CONNEXION_URL")
RAG_API_ASK_QUESTION_URL = os.getenv("RAG_API_ASK_QUESTION_URL")


def _ensure_env(name: str, value: str | None) -> None:
    if not value:
        st.error(f"Variable d'environnement manquante : {name}")
        st.stop()


_ensure_env("RAG_API_TEST_CONNEXION_URL", RAG_API_TEST_CONNEXION_URL)
_ensure_env("RAG_API_ASK_QUESTION_URL", RAG_API_ASK_QUESTION_URL)


# --- FONCTIONS UTILITAIRES ---
def afficher_message(
    role: str,
    llm_response: str,
    retrieved_documents: dict | None = None,
    retrieved_chunks: list[dict] | None = None,
    duration: str | None = None,
    model: str | None = None,
    generated_prompt: list[dict] | None = None,
):
    """Affiche un message + m√©tadonn√©es assistant."""
    with st.chat_message(role):
        st.markdown(llm_response)

        if role == "assistant":
            # infos compactes
            infos = []
            if model:
                infos.append(f"ü§ñ {model}")
            if duration:
                infos.append(f"‚è±Ô∏è {duration}")
            if infos:
                st.caption(" ‚Äî ".join(infos))

            if retrieved_documents:
                with st.expander("üìö Wikis consult√©s"):
                    for title, count in retrieved_documents.items():
                        st.markdown(f"- `{title}` ‚Äî **{count}** extrait(s)")

            if retrieved_chunks:
                with st.expander("üîç Extraits pertinents"):
                    for i, chunk in enumerate(retrieved_chunks, start=1):
                        meta = chunk.get("metadata", {})
                        path = meta.get("path", "Source inconnue")
                        chunk_index = meta.get("chunk_index")
                        similarity = chunk.get("similarity")
                        text_content = chunk.get("document", "Contenu vide")

                        header = f"Extrait {i} ‚Äî {path}"
                        if chunk_index is not None:
                            header += f" (chunk {chunk_index})"
                        if similarity is not None:
                            header += f" ‚Äî sim={similarity:.2f}"

                        with st.expander(header, expanded=(i == 1)):
                            st.markdown(text_content)

            if generated_prompt:
                with st.expander("üß© Prompt g√©n√©r√©"):
                    st.json(generated_prompt)


# --- BARRE LAT√âRALE ---
with st.sidebar:
    image_path = "./assets/images/robot_isilog.png"
    if os.path.exists(image_path):
        st.image(image_path, use_container_width=True)
    else:
        st.title("ü§ñ IsiDore")

    st.caption("LLM bas√© sur la documentation interne ISILOG.")
    st.divider()

    if st.button("üîç √âtat API", use_container_width=True):
        with st.status("Ping API...", expanded=False) as status:
            try:
                response = requests.get(f"{RAG_API_TEST_CONNEXION_URL}/docs", timeout=5)
                if response.status_code == 200:
                    status.update(label="Connect√© ‚úÖ", state="complete")
                else:
                    status.update(
                        label=f"Erreur API ({response.status_code})", state="error"
                    )
            except Exception:
                status.update(label="Serveur injoignable ‚ùå", state="error")

    if st.button("üóëÔ∏è Effacer la discussion", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

# --- HEADER ---
st.title("IsiDore")
st.caption("Interrogez le LLM sur la documentation interne ISILOG.")

# --- INITIALISATION DE L'HISTORIQUE ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- AFFICHAGE DE L'HISTORIQUE ---
for message in st.session_state.messages:
    afficher_message(
        role=message["role"],
        llm_response=message["content"],
        retrieved_documents=message.get("retrieved_documents"),
        retrieved_chunks=message.get("retrieved_chunks"),
        duration=message.get("duration"),
        model=message.get("model"),
        generated_prompt=message.get("generated_prompt"),
    )

# --- ZONE DE CHAT ---
if prompt := st.chat_input("Ex : C'est quoi les Microservices New Way ?"):
    user_msg = {"role": "user", "content": prompt}
    st.session_state.messages.append(user_msg)
    afficher_message("user", prompt)

    with st.spinner("IsiDore r√©fl√©chit..."):
        try:
            payload = {"question": prompt}
            response = requests.post(
                RAG_API_ASK_QUESTION_URL, json=payload, timeout=360
            )

            if response.status_code != 200:
                st.error(f"Erreur API : {response.status_code}")
                st.stop()

            # ‚úÖ Align√© avec AskQuestionResponseBase
            full_response = response.json()

            llm_answer = full_response.get("llm_response", "Pas de r√©ponse g√©n√©r√©e.")
            retrieved_documents = full_response.get("retrieved_documents", [])
            retrieved_chunks = full_response.get("retrieved_chunks", [])
            model = full_response.get("model")
            generated_prompt = full_response.get("generated_prompt")
            duration = full_response.get("duration", "N/A")

            assistant_msg = {
                "role": "assistant",
                "content": llm_answer,
                "retrieved_documents": retrieved_documents,
                "retrieved_chunks": retrieved_chunks,
                "model": model,
                "generated_prompt": generated_prompt,
                "duration": duration,
            }
            st.session_state.messages.append(assistant_msg)

            afficher_message(
                "assistant",
                llm_answer,
                retrieved_documents=retrieved_documents,
                retrieved_chunks=retrieved_chunks,
                duration=duration,
                model=model,
                generated_prompt=generated_prompt,
            )

        except requests.exceptions.Timeout:
            st.error("‚è≥ Le serveur met trop de temps √† r√©pondre.")
        except requests.exceptions.ConnectionError:
            st.error("üîå Impossible de contacter le serveur.")
        except ValueError:
            st.error("R√©ponse API invalide (JSON non parsable).")
        except Exception as e:
            st.error(f"Une erreur est survenue : {e}")
